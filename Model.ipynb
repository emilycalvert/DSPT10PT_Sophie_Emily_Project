{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOmo6DRMzw68FKU5PRIYpo6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Train"],"metadata":{"id":"d8DnMLUppzud"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"id":"4elU4PWomwes","executionInfo":{"status":"error","timestamp":1689375946822,"user_tz":420,"elapsed":186,"user":{"displayName":"Emily Calvert","userId":"10684100702979453534"}},"outputId":"3048e79f-c92a-4035-a9e4-a1307e8bc86b"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b163a3fc8ac5>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Load the datasets with ImageFolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"C:/Users/calve/Downloads/satdata/data/train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n\u001b[0m\u001b[1;32m     27\u001b[0m                                           data_transforms[x])\n\u001b[1;32m     28\u001b[0m                   for x in ['train', 'val']}\n","\u001b[0;32m<ipython-input-2-b163a3fc8ac5>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Load the datasets with ImageFolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"C:/Users/calve/Downloads/satdata/data/train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n\u001b[0m\u001b[1;32m     27\u001b[0m                                           data_transforms[x])\n\u001b[1;32m     28\u001b[0m                   for x in ['train', 'val']}\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/calve/Downloads/satdata/data/train/train'"]}],"source":["import torch\n","import torchvision\n","import torch.optim as optim\n","import torch.nn as nn\n","from torchvision import datasets, models, transforms\n","import os\n","\n","# Define transforms for the training and validation sets\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","# Load the datasets with ImageFolder\n","data_dir = \"C:/Users/calve/Downloads/satdata/data/train\"\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n","                                          data_transforms[x])\n","                  for x in ['train', 'val']}\n","\n","# Define the dataloaders\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n","                                             shuffle=True, num_workers=4)\n","              for x in ['train', 'val']}\n","\n","# Detect if we have a GPU available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load the pre-trained ResNet18 model from torchvision.models\n","model = models.resnet18(pretrained=True)\n","\n","# Replace the final fully connected layer\n","# Parameters of newly constructed modules have requires_grad=True by default\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 4)  # Set the number of output classes. In your case it's 4\n","\n","# Move the model to GPU if available\n","model = model.to(device)\n","\n","# Define the criterion\n","criterion = nn.CrossEntropyLoss()\n","\n","# Observe that all parameters are being optimized\n","optimizer = optim.SGD(model.parameters(), lr=0.001)\n","\n","# Define number of epochs\n","num_epochs = 25\n","\n","# Train the model\n","for epoch in range(num_epochs):\n","    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","    print('-' * 10)\n","\n","    # Each epoch has a training and validation phase\n","    for phase in ['train', 'val']:\n","        if phase == 'train':\n","            model.train()  # Set model to training mode\n","        else:\n","            model.eval()   # Set model to evaluate mode\n","\n","        running_loss = 0.0\n","        running_corrects = 0\n","\n","        # Iterate over data\n","        for inputs, labels in dataloaders[phase]:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            # Forward pass\n","            with torch.set_grad_enabled(phase == 'train'):\n","                outputs = model(inputs)\n","                _, preds = torch.max(outputs, 1)\n","                loss = criterion(outputs, labels)\n","\n","            # Backward and optimize only if in training phase\n","            if phase == 'train':\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","            # Statistics\n","            running_loss += loss.item() * inputs.size(0)\n","            running_corrects += torch.sum(preds == labels.data)\n","\n","        epoch_loss = running_loss / len(image_datasets[phase])\n","        epoch_acc = running_corrects.double() / len(image_datasets[phase])\n","\n","        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","    print()\n","\n","print('Training complete')"]},{"cell_type":"markdown","source":["Validate"],"metadata":{"id":"RkvozkByp4qa"}},{"cell_type":"code","source":["# Load the validation dataset\n","validation_dataset = datasets.ImageFolder(os.path.join(data_dir, 'validation'), test_transforms)\n","validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=4, shuffle=True, num_workers=4)\n","\n","def validate(model, dataloader):\n","    model.eval()  # Set model to evaluate mode\n","    running_corrects = 0\n","    total_samples = 0\n","\n","    # Iterate over data\n","    for inputs, labels in dataloader:\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        total_samples += labels.size(0)\n","\n","        # Forward pass\n","        with torch.set_grad_enabled(False):\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","\n","        # Statistics\n","        running_corrects += torch.sum(preds == labels.data)\n","\n","    accuracy = running_corrects.double() / total_samples\n","\n","    print('Validation Acc: {:.4f}'.format(accuracy))\n","\n","# Call the validation function\n","print(\"Validating the model...\")\n","validate(model, validation_dataloader)"],"metadata":{"id":"ipm2wFofpT9x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test"],"metadata":{"id":"naONc62pqBZF"}},{"cell_type":"code","source":["def test(model, dataloader):\n","    model.eval()  # Set model to evaluate mode\n","    running_corrects = 0\n","    total_samples = 0\n","\n","    # Iterate over data\n","    for inputs, labels in dataloader:\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        total_samples += labels.size(0)\n","\n","        # Forward pass\n","        with torch.set_grad_enabled(False):\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","\n","        # Statistics\n","        running_corrects += torch.sum(preds == labels.data)\n","\n","    accuracy = running_corrects.double() / total_samples\n","\n","    print('Test Acc: {:.4f}'.format(accuracy))\n","\n","# Call the test function\n","print(\"Testing the model...\")\n","test(model, test_dataloader)"],"metadata":{"id":"P2GHDOZnqCiR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Preformance"],"metadata":{"id":"TtAFrielqKdv"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, classification_report\n","import numpy as np\n","\n","def compute_metrics(model, dataloader):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    # Iterate over data\n","    for inputs, labels in dataloader:\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        with torch.set_grad_enabled(False):\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","\n","        # Save all predictions and true labels\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(all_labels, all_preds)\n","    print('Confusion Matrix:')\n","    print(cm)\n","\n","    # Compute classification report\n","    cr = classification_report(all_labels, all_preds, target_names=dataloader.dataset.classes)\n","    print('Classification Report:')\n","    print(cr)\n","\n","# Compute metrics on test set\n","print(\"Computing metrics on test set...\")\n","compute_metrics(model, test_dataloader)"],"metadata":{"id":"7-YgN4RAqPMy"},"execution_count":null,"outputs":[]}]}